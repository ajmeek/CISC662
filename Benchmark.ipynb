{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameek2/CISC662/blob/master/Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IguX8_t3AI8w"
      },
      "source": [
        "Hi y'all. I'm going to use this to put some of the benchmark code in.\n",
        "The below code is taken from Google's notebook introducing TPUs. All it does is check whether or not you have a TPU selected as your hardware accelerator. \n",
        "To set it, go to Runtime->Change runtime settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Go5mnqeAGuv"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM_6Y-SKMix0"
      },
      "source": [
        "Credit:\n",
        "\n",
        "1: https://github.com/EpistasisLab/pmlb\n",
        "\n",
        "2: https://bitbucket.org/berkeleylab/cs-roofline-toolkit/src/master/Empirical_Roofline_Tool-1.1.0/ert \n",
        "\n",
        "Currently going through here and pulling some scripts to run. I'll put them in the code boxes below. For now these will just be some samples from the repo's I'm using as references. We'll pull them together afterward.\n",
        "\n",
        "First is the core loop from perfzero. We can pull different algos from Pytorch and loop over them. Probably one at a time when we analyze in ERT though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgUpxzcBzXP-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "from pmlb import fetch_data, classification_dataset_names\n",
        "\n",
        "logit_test_scores = []\n",
        "gnb_test_scores = []\n",
        "\n",
        "for classification_dataset in classification_dataset_names:\n",
        "    X, y = fetch_data(classification_dataset, return_X_y=True)\n",
        "    train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
        "\n",
        "    logit = LogisticRegression()\n",
        "    gnb = GaussianNB()\n",
        "\n",
        "    logit.fit(train_X, train_y)\n",
        "    gnb.fit(train_X, train_y)\n",
        "\n",
        "    logit_test_scores.append(logit.score(test_X, test_y))\n",
        "    gnb_test_scores.append(gnb.score(test_X, test_y))\n",
        "\n",
        "sb.boxplot(data=[logit_test_scores, gnb_test_scores], notch=True)\n",
        "plt.xticks([0, 1], ['LogisticRegression', 'GaussianNB'])\n",
        "plt.ylabel('Test Accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL6P6ZS1zvwK"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# Import builtin Python modules\n",
        "import sys,os.path\n",
        "\n",
        "# Import ERT python modules\n",
        "from Python.ert_core  import ert_core\n",
        "from Python.ert_utils import *\n",
        "\n",
        "# Get the path to the directory where this was run\n",
        "exe_path = os.path.dirname(sys.argv[0])\n",
        "if exe_path == \"\":\n",
        "  exe_path = \".\"\n",
        "\n",
        "# Create the ERT object\n",
        "ert = ert_core()\n",
        "\n",
        "# Initialize the execution path\n",
        "ert.exe_path = exe_path\n",
        "\n",
        "# Parse the command line\n",
        "if ert.flags() != 0:\n",
        "  sys.stderr.write(\"\\n--- Parsing command line arguments failed --- \\n\\n\")\n",
        "  sys.exit(1)\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Configure the ERT using the configuration file given on the command line\n",
        "if ert.configure() != 0:\n",
        "  sys.stderr.write(\"\\n--- Configuring ERT failed ---\\n\\n\")\n",
        "  sys.exit(1)\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Get the list of flops/element to try (specified by the user)\n",
        "flops_list = parse_int_list(ert.dict[\"CONFIG\"][\"ERT_FLOPS\"][0])\n",
        "\n",
        "# Go through the flops/element list and process each case\n",
        "for flop in flops_list:\n",
        "  # A bit of output if any verbosity has been requested\n",
        "  if ert.options.verbose > 0:\n",
        "    print(\"FLOP count %d...\" % flop)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "  # Set the ERT object flops/element\n",
        "  ert.flop = flop\n",
        "\n",
        "  # Create a directory for these experiments (and give it to the ERT object)\n",
        "  ert.flop_dir = \"%s/FLOPS.%03d\" % (ert.results_dir,flop)\n",
        "  make_dir_if_needed(ert.flop_dir,\"run\",ert.options.verbose > 1)\n",
        "\n",
        "  # Build the current code with the current flops/element\n",
        "  if ert.build() != 0:\n",
        "    sys.stderr.write(\"\\n--- Building ERT failed ---\\n\\n\")\n",
        "    sys.exit(1)\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  # Run the built code over a variety of MPI, OpenMP, and/or CUDA\n",
        "  # configurations specified by the user\n",
        "  if ert.run() != 0:\n",
        "    sys.stderr.write(\"\\n--- Running ERT failed ---\\n\\n\")\n",
        "    sys.exit(1)\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  # Process all the results for the current flops/element\n",
        "  if ert.process() != 0:\n",
        "    sys.stderr.write(\"\\n--- Processing ERT results failed ---\\n\\n\")\n",
        "    sys.exit(1)\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  # Make graphs of all the results for the current flops/element\n",
        "  if ert.graphs() != 0:\n",
        "    sys.stderr.write(\"\\n--- Making ERT individual graphs failed ---\\n\\n\")\n",
        "    sys.exit(1)\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  if ert.options.verbose > 0:\n",
        "    print()\n",
        "\n",
        "# When all the experiments have been processed, generate the roofline graph\n",
        "# and JSON output\n",
        "if ert.roofline() != 0:\n",
        "  sys.stderr.write(\"\\n--- Making ERT roofline failed ---\\n\\n\")\n",
        "  sys.exit(1)\n",
        "\n",
        "# All done\n",
        "sys.stdout.flush()\n",
        "sys.exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrPBoI3yBLY-"
      },
      "source": [
        "Here are some scripts to determine the specs of the GPU/TPU hardware accelerator options. Print these before running, and then we can know where to place them on the roofline model graphs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YDqqrIUBX94",
        "outputId": "afa2814b-6957-4b42-c50c-8675c6f5a57c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 18143965305416487764\n",
              " xla_global_id: -1, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14415560704\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 981142621764829025\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4BOqrk6y7AC3+7g5IAK4F",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}